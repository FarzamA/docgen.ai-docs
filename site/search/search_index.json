{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DocGen.AI","text":"<p>Welcome to the public documentation for DocGen.AI \u2014 a modern, local-first generative AI tool for automatic codebase documentation, unit test generation, and context-aware Q&amp;A.</p> <p>\ud83d\udccc Note: This project is home-hosted. The live demo is generally available Monday - Friday between 9 AM and 5 PM EST. If the link is temporarily down, please refer to this documentation for an overview of features, setup, and usage.</p> <p>\ud83d\udc49 Live Demo Checking...</p>"},{"location":"#overview","title":"\ud83d\udccc Overview","text":"<p>DocGen.AI is a developer-facing tool designed to streamline onboarding and documentation through intelligent code understanding and language models.</p> <p>It supports:</p> <ul> <li>\ud83e\udde0 Generative AI for documentation and test generation</li> <li>\ud83d\udcc2 Secure, local embedding of codebases using open-source models (via Ollama)</li> <li>\ud83e\uddea Unit test scaffolding for existing code</li> <li>\ud83d\udd0d Q&amp;A over your project files \u2014 even without uploading code</li> <li>\ud83d\udd10 SSO authentication with Google, GitHub, Facebook, and Email</li> <li>\ud83c\udf10 Beautiful modern UI with support for dark/light modes</li> <li>\ud83e\uddf1 Dockerized backend + GPU acceleration for inference</li> </ul>"},{"location":"#demo","title":"\ud83d\udcf7 Demo","text":"Live Demo Preview"},{"location":"#docs","title":"\ud83d\udcc1 Docs","text":"<ul> <li>Features</li> <li>Architecture</li> <li>Setup Instructions</li> </ul>"},{"location":"architecture/","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>DocGen.AI combines modern AI infrastructure with a local-first, real-time developer experience optimized for performance, modularity, and privacy.</p>"},{"location":"architecture/#components","title":"Components","text":"<ul> <li>Frontend: React + ShadCN + Tailwind A clean, performant UI powered by Vite, supporting dark mode, dynamic theming, and responsive layout.</li> <li>Backend: FastAPI Handles authentication, chat sessions, codebase management, and serves LLM requests with secure routing.</li> <li>LLM Runtime: Ollama (locally-hosted models) Used to run and stream responses from models like LLaMA 3 and Mistral in a local, containerized environment.</li> <li>Task Queue: ARQ Manages background jobs such as codebase embedding, chunking, token counting, and async file processing with Redis as the queue backend.</li> <li>Realtime Engine: WebSockets (FastAPI) Streams chat messages, background task updates, and live system feedback.</li> <li>Reverse Proxy: Caddy + Docker Compose Handles routing across containers (frontend, backend, Ollama), SSL termination, and proxy rules for API calls.</li> <li>Embeddings + Storage: PostgreSQL + pgvector Stores tokenized code chunks, embedding vectors, and model/chat metadata for RAG-based prompts and retrieval.</li> </ul>"},{"location":"features/","title":"\ud83d\udd0d Features","text":""},{"location":"features/#authentication","title":"\ud83d\udd10 Authentication","text":"<p>DocGen.AI provides a secure and seamless authentication system with support for:</p> OAuth login via Google, Facebook, or Github <p> </p> Email-based registration <p> </p> Email verification for user to be activated  <p> </p> Two-Factor Authentication (2FA) for enhanced security (optional) <p> </p> Email-based password reset <p> </p>"},{"location":"features/#how-it-works","title":"\ud83e\udde0 How It Works","text":"<ul> <li>DocGen.AI connects to your codebase or documentation context.</li> <li>Users can chat with an LLM to generate unit tests, inline documentation, or ask questions about code behavior.</li> <li>You can either connect a GitHub repo or chat with a blank model.</li> <li>The system streams results in real time and provides copyable output.</li> <li>If no codebase is loaded, DocGen.AI defaults to chat-only mode for exploration and experimentation.</li> </ul> Interactive AI Workspace Demo"},{"location":"features/#guest-user-functionality","title":"\ud83d\udc64 Guest User Functionality","text":"<p>DocGen.AI offers guest sessions for quick testing or onboarding:</p> <ul> <li>Guests can explore model features without signing up</li> <li>All guest data (code snippets, chats, preferences) is temporary</li> <li>Guests cannot modify their profiles and have a limited time before their session expires (1 hour)</li> <li>Session data is automatically wiped after logout or timeout</li> </ul>"},{"location":"features/#main-interface","title":"\ud83d\udda5\ufe0f Main Interface","text":"<p>After logging in, users land in a responsive workspace designed for productivity and model experimentation.</p> Sleek dark and light mode themes for visual flexibility <p> </p> Chat-based model interface for documentation, tests, and Q&amp;A <p> </p> Support for chatting with or without a loaded codebase <p> </p>"},{"location":"features/#ai-generated-code-docs","title":"\u270d\ufe0f AI-Generated Code + Docs","text":"<p>The code generation panel allows you to request:</p> <ul> <li>Unit tests for specific functions or classes</li> <li>Inline comments for undocumented logic</li> <li>Refactored or simplified code</li> <li>JSDoc or docstring templates</li> </ul> Code + Test Generation Demo <p> </p>"},{"location":"features/#codebase-chunking-interface","title":"\ud83d\udcc2 Codebase + Chunking Interface","text":"<p>Users can connect external repositories or upload local projects for embedding. DocGen.AI then:</p> <ul> <li>Parses files and chunks content intelligently (e.g. by function)</li> <li>Embeds code for retrieval-augmented generation (RAG)  </li> <li>Displays loading state and import logs</li> <li>Supports reprocessing individual files for updates</li> </ul> Embedding + Chunking Demo <p> </p>"},{"location":"features/#privacy-local-first-deployment","title":"\ud83d\udd10 Privacy + Local-First Deployment","text":"<p>DocGen.AI is designed for privacy-first use cases:</p> <ul> <li>All processing can be done locally or in a containerized environment</li> <li>Supports Ollama and other LLM backends for offline LLM inference  </li> <li>Codebases never leave your machine in local mode </li> <li>Ideal for internal company projects and enterprise security needs Together, these features ensure that trust is not assumed\u2014it\u2019s provable.</li> </ul> Local Deployment with Ollama + GPU Support <p> </p>"},{"location":"features/#models","title":"\ud83e\udde0 Models","text":"<p>DocGen.AI supports multiple generative models via a modular architecture. Models can be used for chat, test generation, documentation, and search \u2014 with flexible backend integration via Ollama, OpenAI, or other compatible APIs.</p> <p>Key features include:</p> <ul> <li>Users can view a searchable list of all available and installed models</li> <li>Usage statistics and last-seen data help track model engagement</li> <li>Tags indicate whether a model is installed locally or just available to pull</li> <li>Status indicators show loading, pulling, or ready state in real time</li> </ul>"},{"location":"features/#admin-only-installation","title":"\ud83d\udd12 Admin-Only Installation","text":"<p>To maintain a secure and controlled environment:</p> <ul> <li>Only admin users can pull or install models to the local environment</li> <li>A model pull dialog allows admins to select specific versions or tags</li> <li>Users without admin privileges will see a disabled or hidden pull button</li> <li>Install requests are tracked and reflected in the UI for transparency</li> </ul> <p>This ensures that the system maintains model consistency and avoids unintentional overload of the host machine.</p> Model Overview + Pulling Interface <p> </p>"},{"location":"setup/","title":"\u2699\ufe0f Setup Instructions","text":"<p>NOTE: This documentation is for demonstration purposes only. The source code for DocGen.AI is currently private.</p>"},{"location":"setup/#requirements","title":"Requirements","text":"<ul> <li>Docker + Docker Compose</li> <li>Ollama installed locally or running via container</li> <li>Redis (used by ARQ for background tasks)</li> <li>Python 3.11+ (for FastAPI and local dev mode)</li> </ul>"},{"location":"setup/#local-deployment-recommended","title":"Local Deployment (Recommended)","text":"<p>To spin up the full stack locally with GPU-enabled LLMs and real-time embedding:</p> <pre><code>git clone https://github.com/FarzamA/DocGen.AI\ncd docgen.ai\ndocker compose up --build\n</code></pre> <p>This will launch:</p> <ul> <li>The React frontend (Vite + ShadCN)</li> <li>FastAPI backend</li> <li>Ollama model server</li> <li>Redis (for background jobs with ARQ)</li> <li>Caddy reverse proxy (serves all apps under one domain)</li> </ul>"},{"location":"setup/#notes","title":"Notes","text":"<ul> <li>GPU support for Ollama is enabled by default; ensure your NVIDIA drivers and CUDA are installed.</li> <li><code>.env</code> file is required in the root directory for configs and Redis connection.</li> <li>For code access, licensing, or enterprise deployment inquiries, please contact the author directly.</li> </ul>"}]}